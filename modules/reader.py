import pandas as pd
from functools import reduce
from pyspark.sql import functions as F
from pyspark.sql.types import StringType
from pyspark.sql.window import Window

def read_and_clean(spark, args):
    df = (
        spark.read.option("header", args.has_header.lower() == "true")
        .option("inferSchema", True)
        .option("delimiter", args.delimiter)
        .csv(args.input)
    )

    # ğŸ”§ XÃ¡c Ä‘á»‹nh cá»™t timestamp
    if args.timecol not in df.columns:
        time_col = df.columns[0]
        print(f"âš ï¸ KhÃ´ng tháº¥y '{args.timecol}', dÃ¹ng '{time_col}'")
        df = df.withColumnRenamed(time_col, "timestamp")
    else:
        df = df.withColumnRenamed(args.timecol, "timestamp")

    # âœ… Ã‰p Ä‘á»‹nh dáº¡ng timestamp kiá»ƒu Má»¹ M/d/yyyy H:mm
    df = df.withColumn("timestamp", F.to_timestamp(F.col("timestamp").cast(StringType()), "M/d/yyyy H:mm"))

    print("ğŸ“Š Kiá»ƒm tra timestamp sau khi Ã©p kiá»ƒu:")
    df.select("timestamp").show(5, truncate=False)
    valid_ts = df.filter(F.col("timestamp").isNotNull()).count()
    print(f"âœ… Timestamp há»£p lá»‡: {valid_ts:,}")

    if valid_ts == 0:
        print("âŒ KhÃ´ng cÃ³ timestamp há»£p lá»‡ â€” kiá»ƒm tra Ä‘á»‹nh dáº¡ng CSV.")
        return df.limit(0)

    sensor_cols = [c for c in df.columns if c != "timestamp"]

    exprs = [
        F.when((F.col(c) <= 0) | (F.col(c) > 120), None)
         .otherwise(F.col(c).cast("double"))
         .alias(c)
        for c in sensor_cols
    ]
    df = df.select("timestamp", *exprs)
    print("ğŸ§¹ LÃ m sáº¡ch dá»¯ liá»‡u thÃ nh cÃ´ng (Ä‘Ã£ loáº¡i bá» giÃ¡ trá»‹ lá»—i & Ã©p kiá»ƒu double).")
    df.show(3)

    # ğŸ” Thá»‘ng kÃª giÃ¡ trá»‹ thiáº¿u theo yÃªu cáº§u
    missing_stats = (
        df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])
          .toPandas()
          .T
          .reset_index()
    )
    missing_stats.columns = ["Cá»™t", "Sá»‘_giÃ¡_trá»‹_thiáº¿u"]
    total_rows = df.count()
    if total_rows == 0:
        print("âš ï¸ DataFrame trá»‘ng sau lÃ m sáº¡ch.")
        return df

    missing_stats["Tá»‰_lá»‡_%"] = (missing_stats["Sá»‘_giÃ¡_trá»‹_thiáº¿u"] / total_rows * 100).round(2)

    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng: {total_rows:,}")
    print("ğŸ” Top 10 cá»™t cÃ³ nhiá»u giÃ¡ trá»‹ thiáº¿u nháº¥t:")
    print(missing_stats.sort_values("Sá»‘_giÃ¡_trá»‹_thiáº¿u", ascending=False).head(10).to_string(index=False))

    # ğŸ§¹ Xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u báº±ng trung bÃ¬nh tá»«ng cá»™t
    print("ğŸ” Äang tÃ­nh giÃ¡ trá»‹ trung bÃ¬nh cho cÃ¡c cá»™t...")
    mean_dict = {}
    for c in df.columns:
        if c != "timestamp":
            mean_val = df.select(F.mean(F.col(c))).collect()[0][0]
            if mean_val is not None:
                mean_dict[c] = float(mean_val)

    print(f"âœ… ÄÃ£ tÃ­nh trung bÃ¬nh cho {len(mean_dict)} cá»™t cÃ³ dá»¯ liá»‡u há»£p lá»‡.")
    df = df.fillna(mean_dict)

    null_count = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])
    print("ğŸ“‹ Sá»‘ giÃ¡ trá»‹ thiáº¿u sau khi fillna:")
    null_count.show(truncate=False)
    print("âœ… ÄÃ£ xá»­ lÃ½ xong toÃ n bá»™ giÃ¡ trá»‹ thiáº¿u.")

    # âš™ï¸ TÃ­nh tá»‘c Ä‘á»™ trung bÃ¬nh & Ä‘áº·c trÆ°ng thá»i gian
    sensor_cols = [c for c in df.columns if c != "timestamp"]
    if not sensor_cols:
        print("âš ï¸ KhÃ´ng cÃ³ cá»™t cáº£m biáº¿n nÃ o Ä‘á»ƒ tÃ­nh avg_speed.")
    else:
        col_exprs = [F.col(c) for c in sensor_cols]
        sum_expr = col_exprs[0] if len(col_exprs) == 1 else reduce(lambda a, b: a + b, col_exprs)
        df = df.withColumn("avg_speed", sum_expr / len(sensor_cols))

    df = df.withColumn("timestamp", F.to_timestamp("timestamp"))
    df = df.withColumn("hour", F.hour("timestamp"))
    df = df.withColumn("weekday", F.dayofweek("timestamp"))

    window = Window.partitionBy("hour")
    df = df.withColumn("std_speed_hour", F.stddev_samp("avg_speed").over(window))
    df = df.fillna({"std_speed_hour": 0.0})
    df = df.filter(F.col("avg_speed").isNotNull())

    if df.rdd.isEmpty():
        print("âš ï¸ KhÃ´ng cÃ²n dá»¯ liá»‡u sau khi tÃ­nh avg_speed.")
        return df

    print("âœ… ÄÃ£ tÃ­nh avg_speed, hour, weekday, std_speed_hour thÃ nh cÃ´ng.")
    df.select("timestamp", "avg_speed", "hour", "weekday", "std_speed_hour").show(10, truncate=False)

    # ğŸ’¥ Loáº¡i bá» ngoáº¡i lai cá»±c máº¡nh (Hard++)
    stats = df.select(
        F.expr("percentile_approx(avg_speed, 0.25)").alias("Q1"),
        F.expr("percentile_approx(avg_speed, 0.75)").alias("Q3")
    ).collect()[0]
    Q1, Q3 = stats.Q1, stats.Q3
    IQR = Q3 - Q1 if Q1 is not None and Q3 is not None else None

    if IQR is None:
        print("âš ï¸ KhÃ´ng thá»ƒ tÃ­nh IQR cho avg_speed.")
    else:
        lower_bound = Q1 - 2.5 * IQR
        upper_bound = Q3 + 2.5 * IQR

        count_before = df.count()
        df_strict = df.filter(
            (F.col("avg_speed") >= lower_bound) &
            (F.col("avg_speed") <= upper_bound) &
            (F.col("avg_speed") > 5) &
            (F.col("avg_speed") < 110)
        )
        count_after = df_strict.count()
        removed = count_before - count_after

        print(f"ğŸ“Š Tá»•ng sá»‘ báº£n ghi ban Ä‘áº§u: {count_before:,}")
        if count_before > 0:
            print(f"ğŸ’£ ÄÃ£ loáº¡i bá» máº¡nh (Hard++): {removed:,} báº£n ghi ({removed / count_before * 100:.2f}%)")
        print(f"âœ… CÃ²n láº¡i: {count_after:,} báº£n ghi há»£p lá»‡")

        df_strict.select(
            F.mean("avg_speed").alias("Trung_bÃ¬nh"),
            F.min("avg_speed").alias("Tá»‘i_thiá»ƒu"),
            F.max("avg_speed").alias("Tá»‘i_Ä‘a")
        ).show()

        df = df_strict

    total = df.count()
    print(f"ğŸ§¼ Sau lÃ m sáº¡ch: cÃ²n {total:,} dÃ²ng")

    cleaned_out_path = f"{args.out}/cleaned_data"
    (
        df.orderBy("timestamp")
          .coalesce(1)
          .write.mode("overwrite").csv(cleaned_out_path, header=True)
    )
    print(f"ğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½ táº¡i {cleaned_out_path}")
    return df
